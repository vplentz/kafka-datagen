{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a38fec-a5dc-423b-b3fd-455f04b65d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/06 13:52:17 WARN Utils: Your hostname, vplentz-computer resolves to a loopback address: 127.0.1.1; using 192.168.1.19 instead (on interface wlp2s0)\n",
      "22/06/06 13:52:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.15\n",
      "Branch HEAD\n",
      "Compiled by user hgao on 2022-01-20T19:26:14Z\n",
      "Revision 4f25b3f71238a00508a356591553f2dfa89f8290\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "! pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ab5bd5-9aac-4ffd-97f2-7b585532a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, sum, window, to_json, struct\n",
    "from pyspark.sql.types import *\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48faff06-a7f7-4435-af95-9c2236aa6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder.appName('ClickStreamUsersStreamMetrics').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2bec9-b7c4-447d-9cb3-ee692d4f1b5c",
   "metadata": {},
   "source": [
    "### With the variable starting offsets you can control if you want to process data from topics beggining or from latest messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40cb5b4d-f1a0-4956-a4cb-a93d527278dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_offsets = 'latest'\n",
    "# starting_offsets = 'earliest'\n",
    "\n",
    "pageviews_stream_df = spark_session.readStream.format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "    .option('subscribe', 'pageviews')\\\n",
    "    .option(\"startingOffsets\", starting_offsets)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f9005-93d1-42b0-8fa0-abd85f201021",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set pageviews schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f25f186-ccc8-4a38-a19c-2c8b53031f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pageviews_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21906be-d0e1-4ca8-a34b-e0c51b5ad17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('viewtime', IntegerType()),\n",
    "    StructField('pageid', StringType()),\n",
    "    StructField('userid', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016aa402-5ec8-4c90-9f71-8bcf6b6d6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviews_stream_df = pageviews_stream_df.select(\n",
    "    col('key').cast(\"string\"),\n",
    "    from_json(col('value').cast('string'), schema).alias('value'),\n",
    "    col('topic'), col('partition'),\n",
    "    col('offset'), col('timestamp'),\n",
    "    col('timestampType')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e34305-96c5-4444-80f8-62d5adb0994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- viewtime: integer (nullable = true)\n",
      " |    |-- pageid: string (nullable = true)\n",
      " |    |-- userid: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pageviews_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290acd36-eb98-4953-a3b5-336ea88965be",
   "metadata": {},
   "source": [
    "### Pageviews as minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06775afa-4d12-4b54-a34f-94396975cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviews_stream_df = pageviews_stream_df.withColumn('value.viewtime', col('value.viewtime')/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f8cfa-e3fc-458a-a6f6-722e88d32aa6",
   "metadata": {},
   "source": [
    "### Processing how many pageviews a user has per minute (for user 1, 2 and 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "775a787a-abc1-4ce4-b25c-08218b4d1692",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list = ['User_1', 'User_2', 'User_3'] \n",
    "users_views_per_minute_df = pageviews_stream_df.filter(col('value.userid').isin(users_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc1ec26-d759-4774-9091-ea07a45f9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING THE PROCESSING TIME, WE HAVE NO EVENT TIME IN THIS DATASET\n",
    "users_views_per_minute_df = users_views_per_minute_df.withWatermark(\"timestamp\", \"1 minutes\").\\\n",
    "groupby(col('value.userid'), window(\"timestamp\", \"1 minute\").alias('grouped_minute')).\\\n",
    "agg(sum('value.viewtime').alias('sum_viewtime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61a88542-9b31-4809-8a61-4125095470f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7febc7e23d60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_views_per_minute_df.select(\n",
    "    to_json(struct(\"userid\", \"grouped_minute\")).alias(\"key\"),\n",
    "    col('sum_viewtime').cast('string').alias('value')).writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .trigger(processingTime='2 minutes')\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"checkpointLocation\", \"../checkpoints/\") \\\n",
    "  .option(\"topic\", \"user_pageview_stats\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1c89af-4142-4644-82e6-d480dcff738d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fbf25de7f70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_views_per_minute_df.select('*').writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef617b3-479f-4a21-b467-054036e6ccd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
